---
title: "Points (or point-pairs) per lag-distance class - PPL"
author: "Alessandro Samuel-Rosa"
date: "24/02/2015"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
  word_document: default
---

# Introduction

It is long known that the variogram is central to geostatistics (Matheron1969;
WebsterEtAl2007). As such, several methods have been proposed to create
sample configurations to estimate the variogram efficiently (deGruijterEtAl2006;
Mueller2007; WebsterEtAl2013). The first methods were based on variogram 
estimation using the classic method of moments (BreslerEtAl1982; Russo1984;
WarrickEtAl1987). Modern methods focus on the robust maximum likelihood
estimators of the variogram (Lark2002; Zimmerman2006; Mueller2007). The later
usually require some prior knowledge about the variogram model.

More recently a Bayesian approach has been suggested to enable taking into
account the uncertainty of the estimated variogram model parameters when
selecting sample locations (ZhuEtAl2005; DiggleEtAl2006). This approach requires
the *a priori* specification of the spatial correlation function and the
interval of possible values for its parameters.

Although theoretically sound, the Bayesian approach is sub-optimal when we
are very uncertain about the variogram model parameters. The less we know, the
less efficient a Bayesian sample configuration is in estimating the true 
variogram model (ZhuEtAl2005). It is a quite efficient approach when we are 
uncertain only about the nugget and/or the partial sill. But when we are 
uncertain about the
range parameter or about the three parameters altogether, its efficiency can be
similar to that of a regularly spaced sample configuration (ZhuEtAl2005).

An adaptive Bayesian approach was suggested as an alternative to overcome the
weaknesses of the single-phase Bayesian approach (MarchantEtAl2006). In adaptive
Bayesian sampling one starts with a reconnaissance survey and, as knowledge
about the spatial variable is constructed, the sample configuration is adapted
iteratively to maximize its efficiency given the chosen optimizing criterion.
Alike its single-phase counterpart, the adaptive Bayesian approach is 
computationally intensive and requires a minimum previous knowledge about the
form of the variogram model.

Practical implementation of the Bayesian methods mentioned above is difficult
if the sample configuration needs to be optimized for more than a couple of 
spatial variables. Also, due to several reasons, we usually are unable or 
uncomfortable to specify the variogram model and the respective ranges of 
possible parameters. As
such, we propose a new method that avoids complicated and time-consuming
calculations. The optimization of the sample configuration relies on the main
assumption that we are completely ignorant about the spatial variation of the
spatial variables.

# Relevant pairwise distances

Sampling to identify the variogram model and estimate its parameters should
concentrate on the relevant pairwise distances. MuellerEtAl1999
observed that the distribution of pairwise distances of the best sample 
configurations had a mode close to the range parameter of the variogram model. 
Lark2002 showed that when the range of spatial dependency is short, the 
resulting distribution of pairwise distances is bimodal, with the first mode 
close to zero distance units, and the second mode at about half the maximum 
distance across the spatial domain.

The literature on sampling for variogram estimation, including the 
studies cited above, provides the basis for the definition of a few 
rules-of-thumb. In general, if we know that a significant part of the variation
is structured at short distances, then several clusters of sample points should
be placed throughout the spatial domain. The size of the clusters, i.e. the
number of sample points in each cluster, depends on the degree of 
autocorrelation of the spatial variable. With smaller spatial dependency,
the sample points should be assigned to a few large clusters to obtain a
reliable estimate of the nugget variance. Smaller clusters can be used when the
degree of spatial dependency is larger. This will yield a more homogeneous
coverage of the spatial domain, although this is not necessary for variogram 
estimation if we assume a constant spatial mean and variance. If both the 
degree and range of spatial dependency are large, then sample points can be 
spread throughout the spatial domain, but retaining a few clusters to estimate
the nugget variance.

## A conservative solution

How do we concentrate on the relevant pairwise distances if we are completely
ignorant about the variation of the spatial variable?
BreslerEtAl1982, Russo1984, and WarrickEtAl1987 proposed the use of a general
purpose, *conservative solution*. According to WarrickEtAl1987, the location of
the sample points should be determined as to match a uniform frequency 
distribution of pairwise distances. In other words, to cover uniformly the
variogram space. Being a conservative solution, it was implicitly sub-optimal
for a given variogram model (MuellerEtAl1999; Lark2002), but expected to be
globally optimal for an infinite set of unknown variogram models. 

At first, the idea of having a uniform distribution of pairwise distances
seems to be appropriate, specially when the number of affordable sample points
is small. Conservative solutions are usually chosen when the resources
available are limited. The same logic is used to optimize sample configurations
to select between a linear and a non-linear structure to model the deterministic
part of the spatial variation of a spatial variable. We sample uniformly
along the marginal distribution of each spatial covariate 
(MinasnyEtAl2006b).

### Critiques and solutions

The method proposed by WarrickEtAl1987 has been criticized for several
reasons. The first critique is due to the fact that the hypothesized global
optimality of the method for an infinite set of unknown variogram models has not
been proved mathematically nor corroborated by empirical evidence (Lark2002).

A common critique to the ideas of BreslerEtAl1982, Russo1984, and 
WarrickEtAl1987 is that they were rooted on the use of the method-of-moments
to fit a continuous function to the binned empirical variogram. It is known that
the estimates of the method-of-moments are affected by the correlation between
the sequence of classes of pairwise distances and that more robust methods
exist (DiggleEtAl2002). Maximum likelihood methods estimate model parameters
using all data points, avoiding the need for an *ad hoc* definition of classes
of pairwise distances that generally smooth out the structure of the spatial
process (Lark2000).

Although being an interesting description of a sample configuration, it is 
known that
sample configurations with the same distribution of pairwise distances can have
completely different spatial configurations (MuellerEtAl1999). This occurs for
two reasons. First, the distribution of pairwise distances is an unidimensional
measure that does not consider the shape and orientation of the spatial domain
being sampled. Second, the position of every sample point is defined relative to
only a fraction of the whole set of sample points. This can ultimately result
in significantly different estimates of the variogram model parameters if our
assumption of a constant spatial mean and variance is incorrect.

The way how the location of every sample point is defined has another negative
consequence. The optimized sample configuration is commonly formed by a single 
large 
cluster of sample points, with a few points scattered throughout the spatial 
domain. We say that the sample configuration is highly redundant, that is, 
poorly
informative. This is particularly inappropriate for variogram estimation due to 
the high correlation between the estimates of the variogram model parameters
(MuellerEtAl1999).

A solution to maximize the amount of information produced by a sample 
configuration
is to divide the spatial domain into equal-size strata within which we 
distribute the sample points (PettittEtAl1993; deGruijterEtAl2006). Because
PettittEtAl1993 also wanted to sample all possibly relevant spatial scales, they
distributed the sample points along transects separated by exponentially growing
distances (0.25, 1, 5, 25, and 125 metres). This is a safe sampling solution
when no information on the magnitude or scale of variation of the spatial
variables is available.

The method proposed by PettittEtAl1993 has another important feature. The use of
exponential distances yields a cluster of points separated by short distances
within each stratum. The use of spatial strata guarantees that there will be
clusters of points spread throughout the entire spatial domain. This is
particularly important because it enables a more accurate estimation of the
nugget variance. The only weakness, aiming at the construction of an
optimization algorithm, is that it is unknown which criterion the method
minimizes or maximizes, being essentially heuristic.

# Points per lag-distance class

We propose a general purpose objective function to optimize sample 
configurations for
the estimation of variogram parameters when we are ignorant about the spatial
variation of the spatial variables. In such a scenario, we can only explore
a distance measure. WarrickEtAl1987 showed that distance-related parameters
control the spread of points and the coverage of the spatial domain. Many later
studies that assumed the form of the variogram model to be known observed that
the range parameter plays a large control of the spatial configuration of the
sample, specially its clustering degree (MuellerEtAl1999; Lark2002; 
ZhuEtAl2005; Zimmerman2006).

The distribution of pairwise distances is a useful description of a sample
configuration. However, its use is rooted on the use of the method-of-moments 
for variogram estimation. Instead of point-pairs, maximum likelihood methods
estimate model parameters using all data points. To maximize the amount of 
information of the data points, we need every sample point to produce 
information for every possibly relevant spatial scale. As such, we need the
location of each sample point to be defined relative to every other sample
point. We also need to define the possibly relevant spatial scales. The logical 
solution is to explore exponential spacings to enable an accurate estimate of 
the nugget variance, but also cover the entire variogram space.

The upper limit of the variogram space should be defined as half the maximum
distance across the spatial domain (TruongEtAl2013). This is because the
estimates of the variogram for distances larger than half the maximum distance
across the sampling grid are highly uncertain (Lark2002). The possibly relevant
spatial scales can be defined exploring the geostatistical concept of 
lag-distance classes. Using exponential spacings up to half the maximum
distance across the spatial domain, and defining the location of every sample 
point relative to every other sample point, will create a sample configuration with 
small clusters scattered throughout the spatial domain.

The proposed method is defined by the following objective: having all sample 
points contributing to all possibly relevant spatial scales. The method is
implemented in the function `optimPPL()` of the R package ***spsann***. The 
method
proposed by WarrickEtAl1987 is implemented as well for comparison purposes, and
can be used setting `pairs = TRUE`. We now describe the definitions used in
the current implementation.

## Distances

The definition of a distance measure is fundamental in our method. The current
implementation uses Euclidean distances between the sample points. This requires
the coordinates to be projected. The user is responsible for making sure that
this requirement is attained. Contributions to allow the use of non-projected 
coordinates are welcome.

## Types of lags

Our interest is on the use of exponentially growing lag-distance classes.
However, we also implemented uniformly spaced lags for comparison purposes. The 
choice between one and another is made using the argument `lags.type`.

Uniformly spaced lags are used setting `lags.type = "equidistant"`. They are
created by simply dividing the interval from 0.0001 up to the value passed to
the argument `cutoff` by the required number of lags. The minimum value of 
0.0001 guarantees that a point does not form a pair with itself when we compute 
pairwise distances. The argument
`cutoff` corresponds to the maximum separation distance up to which the 
lag-distance classes are defined. We recommend the cut-off to be set to half the 
maximum distance across the spatial domain.

The exponentially spaced lag-distance classes are used setting 
`lags.type = "exponential"`. The spacings are defined by the base $b$ of the
exponential expression $b^n$, where $n$ is the required number of lags. The base
is defined using the argument `lags.base`. For example, the default 
`lags.base = 2` creates lags that are sequentially defined as half of the
immediately preceding larger lag. If `cutoff = 100` and `lags = 4`, the limits
of the lag-distance classes will be as follows:

```r
lags.base <- 2
lags <- 4
idx <- lags.base ^ c(1:lags - 1)
cutoff <- 100
c(0.0001, rev(cutoff / idx))
```

## Criteria

The main goal of our method is to produce a sample configuration with a uniform 
distribution of the number of points per lad-distance class (**PPL**). This 
can be achieved in our implementation setting `criterion = "distribution"`. 
Mathematically speaking, we minimize the sum of differences between a 
pre-specified wanted distribution and the observed distribution of points (or 
point-pairs, if `pairs = TRUE`) per lag-distance class. Consider that we aim at
having the following distribution of points per lag:

```r
goal <- c(10, 10, 10, 10, 10)
```

and that the observed distribution of points per lag is as follows:

```r
obs <- c(2, 2, 5, 10, 10)
```

The criterion value is:

```r
sum(goal - obs)
[1] 21
```

The objective of the algorithm is, at each iteration, to match the two 
distributions, that is:

```r
obs <- c(10, 10, 10, 10, 10)
sum(goal - obs)
[1] 0

```

This criterion is of the same type as that proposed by WarrickEtAl1987. However,
the same goal can be achieved using another criterion, possibly less 
computationally demanding. We define it as maximizing the minimum number of
points (or point-pairs) observed over all lag-distance classes. It is used
setting `criterion = "minimum"`. Consider that we observe the following
distribution of points per lag in the first iteration:

```r
obs <- c(2, 3, 5, 10, 10)
```

The objective in the second iteration will be to increase the number of points
in the first lag ($n = 2$). Consider that we then have the following resulting
distribution:

```r
obs <- c(5, 2, 5, 10, 10)
```

Now the objective will be to increase the number of points in the second lag
($n = 2$). The optimization continues until it is not possible to increase the
number of points in any of the lags, that is, when:

```r
obs <- c(10, 10, 10, 10, 10)
```

This shows that the result of using `criterion = "minimum"` is similar to that 
of using `criterion = "distribution"`. However, the resulting sample 
configuration can
be significantly different.

We expected to find significant differences in the running time of the algorithm
when using one criterion or another. This is because when we use 
`criterion = "distribution"` we have to perform one arithmetic operation 
(difference) for every lag and then compute the sum of the differences. Using 
`criterion = "minimum"` should be faster because we only have to search for the
smallest value in the vector of counts. After a few tests we concluded that the
difference is not significant, probably due to the limitations of R. Since 
`criterion = "distribution"` is a more sensitive criterion because it takes all
lags into account, convergence is likely to be attained with a smaller number
of iterations. This obviously also depends on the other parameters passed to the
algorithm. Our recommendation is to use `criterion = "distribution"`.

It is important to note that using `criterion = "distribution"` corresponds to a
*minimization* problem. On the other hand, using `criterion = "minimum"` 
corresponds to a *maximization* problem. We solve this inconsistency 
substituting the criterion that has to be maximized by its inverse. For
convenience we multiply the resulting value by a constant -- the wanted number
of points or point-pairs per lag. If we are aiming at the number of points per
lag, the objective function value $f_{points}$ is calculated as follows:

$$
f_{points} = \frac{n}{min(\mathbf{x}) + 1}
$$

where $n$ is the total number of sample points, and $\mathbf{x}$ is the vector 
of counts of points per lag-distance class. If we are aiming at the number of
point-pairs per lag, the objective function value $f_{pairs}$ is calculated as:

$$
f_{pairs} = \frac{l}{min(\mathbf{x}) + 1}
$$

where $l$ is the wanted number of point-pairs per lag-distance class, and
$\mathbf{x}$ is the vector of counts of point-pairs per lag-distance class. The
wanted number of point-pairs per lag $l$ is calculated as follows 
(WarrickEtAl1987):

$$
l = \frac{n \times (n - 1)}{2 \times nl}
$$

where $nl$ is the number of lag-distance classes. This procedure allows us to
define both problems as minimization problems.

## Utopia and nadir points

In theory, we should be able to calculate the exact utopia and nadir points of
**PPL**. If the objective is to have a uniform distribution of points (or 
point-pairs) per lag, the worst case scenario is to have all points (or 
point-pairs) in a single lag. The utopia point would be equal to zero, and 
the nadir point would be obtained directly by evaluating the wanted distribution
of points (or point-pairs) per lag.

However, there are many aspects that influence the definition of the utopia and
nadir points. The most important is the number of sample points. In general, the
larger the number of sample points, the larger the nadir point. This is because
the objective function is based on the number of points (or point-pairs) per 
lag. But the nadir point increases only up to a certain level because, as the
number of sample points increases it becomes more difficult to have all points
(or point-pairs) in the same lag.

The utopia point becomes closer to zero with the decrease of the number of
sample points. With a too large sample, it becomes more difficult to evenly
distribute the sample points (or point-pairs) among lags. This is specially
significant if the size of the lags is small, showing that there also is an
influence of the size of the various lag-distance classes.

A third element that must be considered is the size and shape of the spatial 
domain. First, because it is intimately related with the definition of the lags
and the amount of random perturbation (jittering) allowed for every point. 
Second, because a sample 
can only be defined as large or small relative to the spatial domain. The third
influence of the spatial domain appears when the ratio between its major and 
minor axes is considerably large. This restricts the set of candidate locations
for the sample points, resulting in many lags not being large enough to 
accommodate the wanted number of points (or point-pairs).

# Examples

## First impression

We present an example using the `meuse` dataset contained in the R-package 
***sp*** (BivandEtAl2008).

```{r}
# load required packages and datasets
require(pedometrics)
require(spsann)
require(sp)
require(rgeos)
data(meuse.grid)
```

In this example we use the object `meuse.grid` to define the set of candidate 
locations for the sample points (`candi`). We also use the `meuse.grid` to
define the boundary of the spatial domain (`boundary`). Note that it is
necessary to add a column to `candi` with the indexes of each candidate 
location.

```{r}
# Define the candidate locations and boundary of the spatial domain
candi <- meuse.grid[, 1:2]
coordinates(candi) <- ~ x + y
gridded(candi) <- TRUE
boundary <- as(candi, "SpatialPolygons")
boundary <- gUnionCascaded(boundary)
candi <- coordinates(candi)
candi <- matrix(cbind(1:nrow(candi), candi), ncol = 3)
```

We now set the arguments that control the amount of random perturbation in the 
x and y coordinates (jittering). The maximum jittering in the x- and 
y-coordinates, given bellow by `x.max` and `y.max`, is defined using the 
bounding box of the spatial domain. This means that the values attributed to
each coordinate are not necessarily the same. The minimum jittering for both 
coordinates (`x.min` and `y.min`) is defined as equal to the grid spacing 
(40 m). The lag-distance cut-off (`cutoff`) is set as equal to the radius of 
the bounding box of the spatial domain.

```{r}
# Jittering settings
x.max <- diff(bbox(boundary)[1, ])
y.max <- diff(bbox(boundary)[2, ])
cutoff <- sqrt((x.max * x.max) + (y.max * y.max)) / 2
```

We use seven exponentially spaced lag-distance classes (`lags.type` and `lags`),
sequentially defined as half the immediately larger preceding lag (`lags.base`).
Our goal is to have a uniform distribution (`criterion`) of points per lag and 
use 1000 `iterations` to let the algorithm achieve it. We define the random seed
so that we can reproduce the results. The starting system configuration is
composed by 100 randomly located `points`.

```{r}
# Run the algorithm
set.seed(2001)
res <- optimPPL(points = 100, candi = candi, lags = 7, cutoff = cutoff,
                x.max = x.max, x.min = 40, y.max = y.max, y.min = 40, 
                boundary = boundary, iterations = 1000)
```

There are two other important functions implemented in the ***spsann*** package:
`countPPL()`, which counts the number of points (or point-pairs) per lag, and
`objPPL()`, which computes the objective function value for a given system
configuration. We see that the number of iterations was not enough to obtain an
uniform distribution of points per lag. There are only 65 points contributing
to the first lag class. The objective function value is somewhat high (65)
since we expected it to be close to zero.

```{r}
# Evaluate the resulting system configuration
countPPL(points = res, lags = 7, cutoff = cutoff)
objPPL(points = res, lags = 7, cutoff = cutoff)
rm(list = ls())
```

## Comparisson with other sample configurations

This example was used to prepare an abstract to the [Pedometrics 2015][ref01], 
which takes place in Córdoba, Spain, between 14 and 18 of September. We compare
a sample configuration obtained using the number of points per lag as the
optimization criterion with other sample configurations: random and systematic,
and a sample configuration optimized aiming at the distribution of the number 
of point-pairs per lag. Our objective is to see which sample configuration is
more efficient in capturing the true form of the data generating process. We do
so sampling form simulated realities and then fitting linear mixed models with
the sampled data. The best performing sample configuration is that for which the 
estimated variogram model parameters are closer to the true variogram model
parameters.

```{r, echo=FALSE, message=FALSE}
# Clean workspace, load required packages, and source user-defined functions
# Load simulated realities created in a previous R section
# All auxiliary files are stored in the tmp folder
rm(list = ls())
gc()
require(pedometrics)
require(spsann)
require(sp)
require(rgeos)
require(geoR)
require(lattice)
require(gridExtra)
source("tmp/ppl.R")
load("tmp/ppl-sim-real.rda")
```

The spatial domain is a square of 500 by 500 (unitless). The spacing of the 
sampling grid is equal to 1 (unitless). The maximum jittering in the x- and 
y-coordinates in the first iteration is equal to the sides of the bounding box
of the spatial domain (500). The minimum jittering for both coordinates in the
first iteration is equal to the square root of the sum of the squares of the
grid spacing in the x- and y-coordinates (see the [Pythagorean theorem][ref02]). 
This guarantees that the point can be shifted to any of the eight nearest
neighbouring points. We use seven exponentially spaced lag-distance classes. The
lag-distance cut-off is equal to the [circumradius][ref03] of the bounding box
of the spatial domain. The solution to the optimization problem has to be found
with a maximum of 50000 iterations.

```{r}
# Generate the spatial domain and sampling grid
# Set the parameters for the optimization
nx <- ny <- 500
s1 <- 1:nx
s1 <- s1 - 0.5
s2 <- s1
candi <- expand.grid(s1 = s1, s2 = s2)
coordinates(candi) <- ~ s1 + s2
boundary <- bbox2sp(candi, keep.crs = FALSE)
candi <- coordinates(candi)
candi <- matrix(cbind(1:nrow(candi), candi), ncol = 3)
x.max <- diff(bbox(boundary)[1, ])
y.max <- diff(bbox(boundary)[2, ])
diago <- sqrt((x.max * x.max) + (y.max * y.max))
cutoff <- diago / 2
lags <- 7
iter <- 100000
seed <- 2000
```

Before we run the optimization algorithm, we simulate (unconditionally) nine
realities. These realities consist of autocorrelated isotropic Gaussian random
fields. The variogram model parameters used to simulate these realities are
described bellow. Their definition is based on the study of 
[Lark (2002)][ref04]. The range of spatial dependence of the variogram model
goes from very short (5 units) to very long (500 units). We do not expect any
sample configuration to accurately estimate the true range parameter of 500
units since estimates beyond the circumradius of the spatial domain are highly
uncertain. Our goal is to see the behaviour of the different sample 
configurations in such an unfavorable case. The quantity of spatially structured
variance (partial sill) also varies from very little (0.1) to very much (0.9).

```{r}
# Simulate realities
# Variogram model parameters based on the study of Lark (2002)
model <- "exponential"
range <- c(5, 50, 500)
p_sill <- c(0.1, 0.5, 0.9)
models <- cbind(model = rep(model, 9),
                expand.grid(p_sill = p_sill, range = range))
models$nugget <- 1 - models$p_sill
models$model <- as.character(models$model)
sim_real <- simulate_realities(models = models, grid = candi[, 2:3])
plot_simulate_realities(sim_real)
```

```{r, echo=FALSE, message=FALSE}
# Save the simulate realities in the tmp folder
save(sim_real, file = "tmp/ppl-sim-real.rda")
```

Now we create the four sample configurations. Because we want to have more 
confidence about the results, we create the four sample configurations three
times setting a different random seed each time. If we had replicated each
sample configuration many more times, then we could find (numerically) the 
approximate confidence intervals for the variogram models and its parameters.
This was the approach adopted by [Webster and Oliver (1992)][ref05], the 
difference being that they used the same sample configuration (a regular grid)
and moved it over the study area. We also use three sample sizes to see if the
number of sample points has any influence on the results.

We use systematic nonaligned sample configurations because there is randomness 
in the selection of the point locations, which guarantees that the three sample
configurations will be different. This type of sample also produces a few
clusters of points, which is important to estimate the nugget variance. The
resulting samples are checked visually and by computing the number of points and
point-pairs per lag-distance class. The last is important to see if the
optimization algorithm converged to the optimum solution in all cases.

```{r}
# Optimize the sample configurations for three sample sizes
# Save the samples for later use
n <- 3
pts <- c(50, 100, 200)
for (i in 1:length(pts)) {
  sample_a <- optim_many_samples(pts = pts[i], lags = lags, iter = iter, 
                                 n = n, seed = seed)
  sample_b <- optim_many_samples(pts = pts[i], lags = lags, iter = iter, 
                                 n = n, pairs = TRUE, seed = seed)
  sample_c <- many_spsamples(pts = pts[i], n = n, type = "random", 
                             boundary = boundary, seed = seed)
  sample_d <- many_spsamples(pts = pts[i], n = n, type = "nonaligned", 
                             boundary = boundary, seed = seed)
  file <- paste("tmp/ppl_samples_", pts[i], ".rda", sep = "")
  save(sample_a, sample_b, sample_c, sample_d, file = file)
  rm(sample_a, sample_b, sample_c, sample_d)
  gc()
}
```

The algorithm does not converge to the optimum solution when the objective is
to have a uniform distribution of point-pairs per lag distance class. The first
three lag-distance classes are largely under-sampled. This was observed with 
50000 and 100000 iterations for the three sample sizes (50, 100, and 200). The
first lags are largely under-sampled when using random and systematic samples
too.

```{r}
# Check the resulting sample configurations (select a single rda file)
#load("tmp/ppl_samples_50.rda")
#load("tmp/ppl_samples_100.rda")
#load("tmp/ppl_samples_200.rda")
# Points
check_samples(list(sample_a, sample_b, sample_c, sample_d), 
              lags = lags, cutoff = cutoff, candi = candi, pairs = FALSE)
# Pairs
check_samples(list(sample_a, sample_b, sample_c, sample_d), 
              lags = lags, cutoff = cutoff, candi = candi, pairs = TRUE)
# Plot samples
x11()
plot_samples(list(sample_a, sample_b, sample_c, sample_d), n = 200)
```

With the sample configurations at hand, we sample the simulated realities. The 
sampled point data is used to calibrate linear mixed models using the R-package
`geoR`. Model parameters are estimated using restricted maximum likelihood
(REML). Estimated parameters are: nugget, partial sill, and range.

```{r}
# Sample the simulated realities
sample_a <- sample_simulated_realities(samples = sample_a, realities = sim_real)
sample_b <- sample_simulated_realities(samples = sample_b, realities = sim_real)
sample_c <- sample_simulated_realities(samples = sample_c, realities = sim_real)
sample_d <- sample_simulated_realities(samples = sample_d, realities = sim_real)
# Calibrate linear mixed models
sample_a_lmm <- calibrate_lmm(samples = sample_a)
sample_b_lmm <- calibrate_lmm(samples = sample_b)
sample_c_lmm <- calibrate_lmm(samples = sample_c)
sample_d_lmm <- calibrate_lmm(samples = sample_d)
```

Using 50 sample points, both criteria the number of points and point-pairs per
lag enable an accurate estimate of the nugget variance if the range is above 50
m. For a short range (5 m), these criteria are somewhat efficient only when the
nugget variance is very small.
Estimating the partial sill was somewhat difficult. But it appears that using
the number of points per lag makes the better estimates when the partial sill is
moderate to high, and the range is bellow 50 m. The number of point-pairs is
more efficient for low to moderate partial sill when the range is bellow 50 m.
No sample configuration made an accurate estimate of the range when the nugget
variance is high, or when the range is very long (500 m). Using the number of 
points per lag as the optimization criterion was more efficient to estimate 
the range (5 and 50 m) when the nugget variance was modetrate to low.
Both random and systematic samples were inneficient for all evaluated 
parameters.

With 100 samples, the nugget variance was better estimated when using the number 
of points as criterion when the range was moderate to long and the nugget 
small to moderate, and when the nugget was small and the range short. The number
of poinrt-pairs was efficient in estimating the nugget when it was small for all
three ranges.
Again, estimating the partial sill was somwhat difficult. The number of points
was efficient only with large ranges and small to moderate partial sills.
Overall, the range is better estimated when it is short to moderate independent
of the sample configuration.
Systematic samples improved the prediction with increasing the number of points.


With 200 sample, both points and point-pairs make an accurate estimate of the 
nugget when the range is moderate to long. Systematic sample makes precise
estimates, but overstimates the nugget.

Again, estimating the partial sill was somwhat difficult. Curiously, the 
systematic sample was the one that performed best when the range was moderate.
The number of point-pairs was consistently better when the partial sill was 
high. The number of points was consistently better when the partial sill was 
low.

Overall, the range is better estimated when it is short to moderate independent
of the sample configuration.

```{r}
# Compare the estimated parameters with the true parameters
sample_a_lmm <- data.frame(sample = "points", models, sample_a_lmm)
sample_b_lmm <- data.frame(sample = "pairs", models, sample_b_lmm)
sample_c_lmm <- data.frame(sample = "random", models, sample_c_lmm)
sample_d_lmm <- data.frame(sample = "systematic", models, sample_d_lmm)
sample_all <- rbind(sample_a_lmm, sample_b_lmm, sample_c_lmm, sample_d_lmm)
sample_all <- sample_all[, -2]
x11()
n <- 200
dev.off()
pdf(paste("tmp/nugget-", n, "pts.pdf", sep = ""))
plot_nugget(sample_all, n = n)
dev.off()
pdf(paste("tmp/partial-sill-", n, "pts.pdf", sep = ""))
plot_partial_sill(sample_all, n = n)
dev.off()
pdf(paste("tmp/range-", n, "pts.pdf", sep = ""))
plot_range(sample_all, n = n)
dev.off()
```


[ref01]: https://sites.google.com/site/pedometrics2015
[ref02]: https://en.wikipedia.org/wiki/Pythagorean_theorem
[ref03]: https://en.wikipedia.org/wiki/Circumscribed_circle
[ref04]: http://www.sciencedirect.com/science/article/pii/S0016706101000921
[ref05]: http://dx.doi.org/10.1111/j.1365-2389.1992.tb00128.x
