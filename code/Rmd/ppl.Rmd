---
title: "Points (or point-pairs) per lag-distance class - PPL"
author: "Alessandro Samuel-Rosa"
date: "24/02/2015"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
  word_document: default
---

# Introduction

It is long known that the variogram is central to geostatistics (Matheron1969;
WebsterEtAl2007). As such, several methods have been proposed to create
sample configurations to estimate the variogram efficiently (deGruijterEtAl2006;
Mueller2007; WebsterEtAl2013). The first methods were based on variogram 
estimation using the classic method of moments (BreslerEtAl1982; Russo1984;
WarrickEtAl1987). Modern methods focus on the robust maximum likelihood
estimators of the variogram (Lark2002; Zimmerman2006; Mueller2007). The later
usually require some prior knowledge about the variogram model.

More recently a Bayesian approach has been suggested to enable taking into
account the uncertainty of the estimated variogram model parameters when
selecting sample locations (ZhuEtAl2005; DiggleEtAl2006). This approach requires
the *a priori* specification of the spatial correlation function and the
interval of possible values for its parameters.

Although theoretically sound, the Bayesian approach is sub-optimal when we
are very uncertain about the variogram model parameters. The less we know, the
less efficient a Bayesian sample configuration is in estimating the true 
variogram model (ZhuEtAl2005). It is a quite efficient approach when we are 
uncertain only about the nugget and/or the partial sill. But when we are 
uncertain about the
range parameter or about the three parameters altogether, its efficiency can be
similar to that of a regularly spaced sample configuration (ZhuEtAl2005).

An adaptive Bayesian approach was suggested as an alternative to overcome the
weaknesses of the single-phase Bayesian approach (MarchantEtAl2006). In adaptive
Bayesian sampling one starts with a reconnaissance survey and, as knowledge
about the spatial variable is constructed, the sample configuration is adapted
iteratively to maximize its efficiency given the chosen optimizing criterion.
Alike its single-phase counterpart, the adaptive Bayesian approach is 
computationally intensive and requires a minimum previous knowledge about the
form of the variogram model.

Practical implementation of the Bayesian methods mentioned above is difficult
if the sample configuration needs to be optimized for more than a couple of 
spatial variables. Also, due to several reasons, we usually are unable or 
uncomfortable to specify the variogram model and the respective ranges of 
possible parameters. As
such, we propose a new method that avoids complicated and time-consuming
calculations. The optimization of the sample configuration relies on the main
assumption that we are completely ignorant about the spatial variation of the
spatial variables.

# Relevant pairwise distances

Sampling to identify the variogram model and estimate its parameters should
concentrate on the relevant pairwise distances. MuellerEtAl1999
observed that the distribution of pairwise distances of the best sample 
configurations had a mode close to the range parameter of the variogram model. 
Lark2002 showed that when the range of spatial dependency is short, the 
resulting distribution of pairwise distances is bimodal, with the first mode 
close to zero distance units, and the second mode at about half the maximum 
distance across the spatial domain.

The literature on sampling for variogram estimation, including the 
studies cited above, provides the basis for the definition of a few 
rules-of-thumb. In general, if we know that a significant part of the variation
is structured at short distances, then several clusters of sample points should
be placed throughout the spatial domain. The size of the clusters, i.e. the
number of sample points in each cluster, depends on the degree of 
autocorrelation of the spatial variable. With smaller spatial dependency,
the sample points should be assigned to a few large clusters to obtain a
reliable estimate of the nugget variance. Smaller clusters can be used when the
degree of spatial dependency is larger. This will yield a more homogeneous
coverage of the spatial domain, although this is not necessary for variogram 
estimation if we assume a constant spatial mean and variance. If both the 
degree and range of spatial dependency are large, then sample points can be 
spread throughout the spatial domain, but retaining a few clusters to estimate
the nugget variance.

## A conservative solution

How do we concentrate on the relevant pairwise distances if we are completely
ignorant about the variation of the spatial variable?
BreslerEtAl1982, Russo1984, and WarrickEtAl1987 proposed the use of a general
purpose, *conservative solution*. According to WarrickEtAl1987, the location of
the sample points should be determined as to match a uniform frequency 
distribution of pairwise distances. In other words, to cover uniformly the
variogram space. Being a conservative solution, it was implicitly sub-optimal
for a given variogram model (MuellerEtAl1999; Lark2002), but expected to be
globally optimal for an infinite set of unknown variogram models. 

At first, the idea of having a uniform distribution of pairwise distances
seems to be appropriate, specially when the number of affordable sample points
is small. Conservative solutions are usually chosen when the resources
available are limited. The same logic is used to optimize sample configurations
to select between a linear and a non-linear structure to model the deterministic
part of the spatial variation of a spatial variable. We sample uniformly
along the marginal distribution of each spatial covariate 
(MinasnyEtAl2006b).

### Critiques and solutions

The method proposed by WarrickEtAl1987 has been criticized for several
reasons. The first critique is due to the fact that the hypothesized global
optimality of the method for an infinite set of unknown variogram models has not
been proved mathematically nor corroborated by empirical evidence (Lark2002).

A common critique to the ideas of BreslerEtAl1982, Russo1984, and 
WarrickEtAl1987 is that they were rooted on the use of the method-of-moments
to fit a continuous function to the binned empirical variogram. It is known that
the estimates of the method-of-moments are affected by the correlation between
the sequence of classes of pairwise distances and that more robust methods
exist (DiggleEtAl2002). Maximum likelihood methods estimate model parameters
using all data points, avoiding the need for an *ad hoc* definition of classes
of pairwise distances that generally smooth out the structure of the spatial
process (Lark2000).

Although being an interesting description of a sample configuration, it is 
known that
sample configurations with the same distribution of pairwise distances can have
completely different spatial configurations (MuellerEtAl1999). This occurs for
two reasons. First, the distribution of pairwise distances is an unidimensional
measure that does not consider the shape and orientation of the spatial domain
being sampled. Second, the position of every sample point is defined relative to
only a fraction of the whole set of sample points. This can ultimately result
in significantly different estimates of the variogram model parameters if our
assumption of a constant spatial mean and variance is incorrect.

The way how the location of every sample point is defined has another negative
consequence. The optimized sample configuration is commonly formed by a single 
large 
cluster of sample points, with a few points scattered throughout the spatial 
domain. We say that the sample configuration is highly redundant, that is, 
poorly
informative. This is particularly inappropriate for variogram estimation due to 
the high correlation between the estimates of the variogram model parameters
(MuellerEtAl1999).

A solution to maximize the amount of information produced by a sample 
configuration
is to divide the spatial domain into equal-size strata within which we 
distribute the sample points (PettittEtAl1993; deGruijterEtAl2006). Because
PettittEtAl1993 also wanted to sample all possibly relevant spatial scales, they
distributed the sample points along transects separated by exponentially growing
distances (0.25, 1, 5, 25, and 125 metres). This is a safe sampling solution
when no information on the magnitude or scale of variation of the spatial
variables is available.

The method proposed by PettittEtAl1993 has another important feature. The use of
exponential distances yields a cluster of points separated by short distances
within each stratum. The use of spatial strata guarantees that there will be
clusters of points spread throughout the entire spatial domain. This is
particularly important because it enables a more accurate estimation of the
nugget variance. The only weakness, aiming at the construction of an
optimization algorithm, is that it is unknown which criterion the method
minimizes or maximizes, being essentially heuristic.

# Points per lag-distance class

We propose a general purpose objective function to optimize sample 
configurations for
the estimation of variogram parameters when we are ignorant about the spatial
variation of the spatial variables. In such a scenario, we can only explore
a distance measure. WarrickEtAl1987 showed that distance-related parameters
control the spread of points and the coverage of the spatial domain. Many later
studies that assumed the form of the variogram model to be known observed that
the range parameter plays a large control of the spatial configuration of the
sample, specially its clustering degree (MuellerEtAl1999; Lark2002; 
ZhuEtAl2005; Zimmerman2006).

The distribution of pairwise distances is a useful description of a sample
configuration. However, its use is rooted on the use of the method-of-moments 
for variogram estimation. Instead of point-pairs, maximum likelihood methods
estimate model parameters using all data points. To maximize the amount of 
information of the data points, we need every sample point to produce 
information for every possibly relevant spatial scale. As such, we need the
location of each sample point to be defined relative to every other sample
point. We also need to define the possibly relevant spatial scales. The logical 
solution is to explore exponential spacings to enable an accurate estimate of 
the nugget variance, but also cover the entire variogram space.

The upper limit of the variogram space should be defined as half the maximum
distance across the spatial domain (TruongEtAl2013). This is because the
estimates of the variogram for distances larger than half the maximum distance
across the sampling grid are highly uncertain (Lark2002). The possibly relevant
spatial scales can be defined exploring the geostatistical concept of 
lag-distance classes. Using exponential spacings up to half the maximum
distance across the spatial domain, and defining the location of every sample 
point relative to every other sample point, will create a sample configuration with 
small clusters scattered throughout the spatial domain.

The proposed method is defined by the following objective: having all sample 
points contributing to all possibly relevant spatial scales. The method is
implemented in the function `optimPPL()` of the R package ***spsann***. The 
method
proposed by WarrickEtAl1987 is implemented as well for comparison purposes, and
can be used setting `pairs = TRUE`. We now describe the definitions used in
the current implementation.

## Distances

The definition of a distance measure is fundamental in our method. The current
implementation uses Euclidean distances between the sample points. This requires
the coordinates to be projected. The user is responsible for making sure that
this requirement is attained. Contributions to allow the use of non-projected 
coordinates are welcome.

## Types of lags

Our interest is on the use of exponentially growing lag-distance classes.
However, we also implemented uniformly spaced lags for comparison purposes. The 
choice between one and another is made using the argument `lags.type`.

Uniformly spaced lags are used setting `lags.type = "equidistant"`. They are
created by simply dividing the interval from 0.0001 up to the value passed to
the argument `cutoff` by the required number of lags. The minimum value of 
0.0001 guarantees that a point does not form a pair with itself when we compute 
pairwise distances. The argument
`cutoff` corresponds to the maximum separation distance up to which the 
lag-distance classes are defined. We recommend the cut-off to be set to half the 
maximum distance across the spatial domain.

The exponentially spaced lag-distance classes are used setting 
`lags.type = "exponential"`. The spacings are defined by the base $b$ of the
exponential expression $b^n$, where $n$ is the required number of lags. The base
is defined using the argument `lags.base`. For example, the default 
`lags.base = 2` creates lags that are sequentially defined as half of the
immediately preceding larger lag. If `cutoff = 100` and `lags = 4`, the limits
of the lag-distance classes will be as follows:

```r
lags.base <- 2
lags <- 4
idx <- lags.base ^ c(1:lags - 1)
cutoff <- 100
c(0.0001, rev(cutoff / idx))
```

## Criteria

The main goal of our method is to produce a sample configuration with a uniform 
distribution of the number of points per lad-distance class (**PPL**). This 
can be achieved in our implementation setting `criterion = "distribution"`. 
Mathematically speaking, we minimize the sum of differences between a 
pre-specified wanted distribution and the observed distribution of points (or 
point-pairs, if `pairs = TRUE`) per lag-distance class. Consider that we aim at
having the following distribution of points per lag:

```r
goal <- c(10, 10, 10, 10, 10)
```

and that the observed distribution of points per lag is as follows:

```r
obs <- c(2, 2, 5, 10, 10)
```

The criterion value is:

```r
sum(goal - obs)
[1] 21
```

The objective of the algorithm is, at each iteration, to match the two 
distributions, that is:

```r
obs <- c(10, 10, 10, 10, 10)
sum(goal - obs)
[1] 0

```

This criterion is of the same type as that proposed by WarrickEtAl1987. However,
the same goal can be achieved using another criterion, possibly less 
computationally demanding. We define it as maximizing the minimum number of
points (or point-pairs) observed over all lag-distance classes. It is used
setting `criterion = "minimum"`. Consider that we observe the following
distribution of points per lag in the first iteration:

```r
obs <- c(2, 3, 5, 10, 10)
```

The objective in the second iteration will be to increase the number of points
in the first lag ($n = 2$). Consider that we then have the following resulting
distribution:

```r
obs <- c(5, 2, 5, 10, 10)
```

Now the objective will be to increase the number of points in the second lag
($n = 2$). The optimization continues until it is not possible to increase the
number of points in any of the lags, that is, when:

```r
obs <- c(10, 10, 10, 10, 10)
```

This shows that the result of using `criterion = "minimum"` is similar to that 
of using `criterion = "distribution"`. However, the resulting sample 
configuration can
be significantly different.

We expected to find significant differences in the running time of the algorithm
when using one criterion or another. This is because when we use 
`criterion = "distribution"` we have to perform one arithmetic operation 
(difference) for every lag and then compute the sum of the differences. Using 
`criterion = "minimum"` should be faster because we only have to search for the
smallest value in the vector of counts. After a few tests we concluded that the
difference is not significant, probably due to the limitations of R. Since 
`criterion = "distribution"` is a more sensitive criterion because it takes all
lags into account, convergence is likely to be attained with a smaller number
of iterations. This obviously also depends on the other parameters passed to the
algorithm. Our recommendation is to use `criterion = "distribution"`.

It is important to note that using `criterion = "distribution"` corresponds to a
*minimization* problem. On the other hand, using `criterion = "minimum"` 
corresponds to a *maximization* problem. We solve this inconsistency 
substituting the criterion that has to be maximized by its inverse. For
convenience we multiply the resulting value by a constant -- the wanted number
of points or point-pairs per lag. If we are aiming at the number of points per
lag, the objective function value $f_{points}$ is calculated as follows:

$$
f_{points} = \frac{n}{min(\mathbf{x}) + 1}
$$

where $n$ is the total number of sample points, and $\mathbf{x}$ is the vector 
of counts of points per lag-distance class. If we are aiming at the number of
point-pairs per lag, the objective function value $f_{pairs}$ is calculated as:

$$
f_{pairs} = \frac{l}{min(\mathbf{x}) + 1}
$$

where $l$ is the wanted number of point-pairs per lag-distance class, and
$\mathbf{x}$ is the vector of counts of point-pairs per lag-distance class. The
wanted number of point-pairs per lag $l$ is calculated as follows 
(WarrickEtAl1987):

$$
l = \frac{n \times (n - 1)}{2 \times nl}
$$

where $nl$ is the number of lag-distance classes. This procedure allows us to
define both problems as minimization problems.

## Utopia and nadir points

In theory, we should be able to calculate the exact utopia and nadir points of
**PPL**. If the objective is to have a uniform distribution of points (or 
point-pairs) per lag, the worst case scenario is to have all points (or 
point-pairs) in a single lag. The utopia point would be equal to zero, and 
the nadir point would be obtained directly by evaluating the wanted distribution
of points (or point-pairs) per lag.

However, there are many aspects that influence the definition of the utopia and
nadir points. The most important is the number of sample points. In general, the
larger the number of sample points, the larger the nadir point. This is because
the objective function is based on the number of points (or point-pairs) per 
lag. But the nadir point increases only up to a certain level because, as the
number of sample points increases it becomes more difficult to have all points
(or point-pairs) in the same lag.

The utopia point becomes closer to zero with the decrease of the number of
sample points. With a too large sample, it becomes more difficult to evenly
distribute the sample points (or point-pairs) among lags. This is specially
significant if the size of the lags is small, showing that there also is an
influence of the size of the various lag-distance classes.

A third element that must be considered is the size and shape of the spatial 
domain. First, because it is intimately related with the definition of the lags
and the amount of random perturbation (jittering) allowed for every point. 
Second, because a sample 
can only be defined as large or small relative to the spatial domain. The third
influence of the spatial domain appears when the ratio between its major and 
minor axes is considerably large. This restricts the set of candidate locations
for the sample points, resulting in many lags not being large enough to 
accommodate the wanted number of points (or point-pairs).

# Examples

## First impression

We present an example using the `meuse` dataset contained in the R-package 
***sp*** (BivandEtAl2008).

```{r}
# load required packages and datasets
require(pedometrics)
require(spsann)
require(sp)
require(rgeos)
data(meuse.grid)
```

In this example we use the object `meuse.grid` to define the set of candidate 
locations for the sample points (`candi`). We also use the `meuse.grid` to
define the boundary of the spatial domain (`boundary`). Note that it is
necessary to add a column to `candi` with the indexes of each candidate 
location.

```{r}
# Define the candidate locations and boundary of the spatial domain
candi <- meuse.grid[, 1:2]
coordinates(candi) <- ~ x + y
gridded(candi) <- TRUE
boundary <- as(candi, "SpatialPolygons")
boundary <- gUnionCascaded(boundary)
candi <- coordinates(candi)
candi <- matrix(cbind(1:nrow(candi), candi), ncol = 3)
```

We now set the arguments that control the amount of random perturbation in the 
x and y coordinates (jittering). The maximum jittering in the x- and 
y-coordinates, given bellow by `x.max` and `y.max`, is defined using the 
bounding box of the spatial domain. This means that the values attributed to
each coordinate are not necessarily the same. The minimum jittering for both 
coordinates (`x.min` and `y.min`) is defined as equal to the grid spacing 
(40 m). The lag-distance cut-off (`cutoff`) is set as equal to the radius of 
the bounding box of the spatial domain.

```{r}
# Jittering settings
x.max <- diff(bbox(boundary)[1, ])
y.max <- diff(bbox(boundary)[2, ])
cutoff <- sqrt((x.max * x.max) + (y.max * y.max)) / 2
```

We use seven exponentially spaced lag-distance classes (`lags.type` and `lags`),
sequentially defined as half the immediately larger preceding lag (`lags.base`).
Our goal is to have a uniform distribution (`criterion`) of points per lag and 
use 1000 `iterations` to let the algorithm achieve it. We define the random seed
so that we can reproduce the results. The starting system configuration is
composed by 100 randomly located `points`.

```{r}
# Run the algorithm
set.seed(2001)
res <- optimPPL(points = 100, candi = candi, lags = 7, cutoff = cutoff,
                x.max = x.max, x.min = 40, y.max = y.max, y.min = 40, 
                boundary = boundary, iterations = 1000)
```

There are two other important functions implemented in the ***spsann*** package:
`countPPL()`, which counts the number of points (or point-pairs) per lag, and
`objPPL()`, which computes the objective function value for a given system
configuration. We see that the number of iterations was not enough to obtain an
uniform distribution of points per lag. There are only 65 points contributing
to the first lag class. The objective function value is somewhat high (65)
since we expected it to be close to zero.

```{r}
# Evaluate the resulting system configuration
countPPL(points = res, lags = 7, cutoff = cutoff)
objPPL(points = res, lags = 7, cutoff = cutoff)
rm(list = ls())
```

## Comparisson with other sample configurations

This example was used to prepare an abstract to the [Pedometrics 2015][ref01], 
which takes place in CÃ³rdoba, Spain, between 14 and 18 of September. We compare
a sample configuration obtained using the number of points per lag as the
optimization criterion with other sample configurations: random and systematic,
and a sample configuration optimized aiming at the distribution of the number 
of point-pairs per lag. Our objective is to see which sample configuration is
more efficient in capturing the true form of the data generating process. We do
so sampling form simulated realities and then fitting linear mixed models with
the sampled data. The best performing sample configuration is that for which the 
estimated variogram model parameters are closer to the true variogram model
parameters.

```{r, echo=FALSE, message=FALSE}
# Clean workspace, load required packages, and source user-defined functions
# Load simulated realities created in a previous R section
# All auxiliary files are stored in the tmp folder
rm(list = ls())
gc()
require(pedometrics)
require(spsann)
require(sp)
require(rgeos)
require(geoR)
require(lattice)
require(gridExtra)
source("code/R/helper_functions.R")
load("data/R/ppl-sim-real.rda")
```

The spatial domain is a square of 500 by 500 (unitless). The spacing of the 
sampling grid is equal to 1 (unitless). The maximum jittering in the x- and 
y-coordinates in the first iteration is equal to the sides of the bounding box
of the spatial domain (500). The minimum jittering for both coordinates in the
first iteration is equal to the square root of the sum of the squares of the
grid spacing in the x- and y-coordinates (see the [Pythagorean theorem][ref02]). 
This guarantees that the point can be shifted to any of the eight nearest
neighbouring points. We use seven exponentially spaced lag-distance classes. The
lag-distance cut-off is equal to the [circumradius][ref03] of the bounding box
of the spatial domain. The solution to the optimization problem has to be found
with a maximum of 50000 iterations.

```{r}
# Generate the spatial domain and sampling grid
# Set the parameters for the optimization
nx <- ny <- 500
s1 <- 1:nx
s1 <- s1 - 0.5
s2 <- s1
candi <- expand.grid(s1 = s1, s2 = s2)
coordinates(candi) <- ~ s1 + s2
boundary <- bbox2sp(candi, keep.crs = FALSE)
candi <- coordinates(candi)
candi <- matrix(cbind(1:nrow(candi), candi), ncol = 3)
x.max <- diff(bbox(boundary)[1, ])
y.max <- diff(bbox(boundary)[2, ])
diago <- sqrt((x.max * x.max) + (y.max * y.max))
cutoff <- diago / 2
lags <- 7
iter <- 100000
seed <- 2000
```

Before we run the optimization algorithm, we simulate (unconditionally) nine
realities. These realities consist of autocorrelated isotropic Gaussian random
fields. The variogram model parameters used to simulate these realities are
described bellow. Their definition is based on the study of 
[Lark (2002)][ref04]. The range of spatial dependence of the variogram model
goes from very short (5 units) to very long (500 units). We do not expect any
sample configuration to accurately estimate the true range parameter of 500
units since estimates beyond the circumradius of the spatial domain are highly
uncertain. Our goal is to see the behaviour of the different sample 
configurations in such an unfavorable case. The quantity of spatially structured
variance (partial sill) also varies from very little (0.1) to very much (0.9).

```{r, echo=FALSE, message=FALSE}
# Simulate realities
# Variogram model parameters based on the study of Lark (2002)
model <- "exponential"
range <- c(5, 50, 500)
p_sill <- c(0.1, 0.5, 0.9)
models <- cbind(model = rep(model, 9), 
                expand.grid(p_sill = p_sill, range = range))
models$nugget <- 1 - models$p_sill
models$model <- as.character(models$model)
sim_real <- simulate_realities(models = models, grid = candi[, 2:3])
dev.off()
png("res/fig/ppl-sim-real.png", res = 300, height = 150, width = 150, 
    units = 'mm')
plot_simulate_realities(sim_real)
dev.off()
# Save the simulate realities in the Rdata folder
save(sim_real, file = "data/R/ppl-sim-real.rda")
```

Now we create the four sample configurations. Because we want to have more 
confidence about the results, we create the four sample configurations three
times setting a different random seed each time. If we had replicated each
sample configuration many more times, then we could find (numerically) the 
approximate confidence intervals for the variogram models and its parameters.
This was the approach adopted by [Webster and Oliver (1992)][ref05], the 
difference being that they used the same sample configuration (a regular grid)
and moved it over the study area. We also use three sample sizes (50, 100, 200)
to see if the number of sample points has any influence on the results.

We use systematic nonaligned sampling because there is randomness in the
selection of the point locations, which guarantees that the three sample
configurations will be different. This sampling type also produces a few
clusters of points, which is important to estimate the nugget variance. The
resulting samples are checked visually and by computing the number of points and
point-pairs per lag-distance class. The last is important to see if the
optimization algorithm converged to the optimum solution in all cases.

```{r}
# Optimize the sample configurations for three sample sizes
# Save the samples for later use
n <- 3
pts <- c(50, 100, 200)
for (i in 1:length(pts)) {
  sample_a <- optim_many_samples(pts = pts[i], lags = lags, iter = iter, 
                                 n = n, seed = seed)
  sample_b <- optim_many_samples(pts = pts[i], lags = lags, iter = iter, 
                                 n = n, pairs = TRUE, seed = seed)
  sample_c <- many_spsamples(pts = pts[i], n = n, type = "random", 
                             boundary = boundary, seed = seed)
  sample_d <- many_spsamples(pts = pts[i], n = n, type = "nonaligned", 
                             boundary = boundary, seed = seed)
  file <- paste("data/R/ppl_samples_", pts[i], ".rda", sep = "")
  save(sample_a, sample_b, sample_c, sample_d, file = file)
  rm(sample_a, sample_b, sample_c, sample_d)
  gc()
}
```

The box-and-whisker plots show that the algorithm does not converge to the 
optimum solution when the objective is to have a uniform distribution of 
point-pairs per lag distance class. The number of point-pairs in the first two
lag-distance classes is considerably lower than expected for the three sample 
sizes. The same result was observed using 50000 and 100000 iterations. The other
three sample types result in a considerably larger number of point-pairs in 
the last two lag-distance classes.

```{r}
# Check the resulting sample configurations (select a single rda file)
n <- 200
load(paste("data/R/ppl_samples_", n, ".rda", sep = ""))
# Points
plot_ppl_distribution(samples = list(sample_a, sample_b, sample_c, sample_d), 
                      lags = lags, cutoff = cutoff, candi = candi,
                      pairs = FALSE, main = "Points (n = 50)")
# Pairs
plot_ppl_distribution(samples = list(sample_a, sample_b, sample_c, sample_d), 
                      lags = lags, cutoff = cutoff, candi = candi,
                      pairs = TRUE, main = "Pairs (n = 50)")
# Plot samples
dev.off()
png(paste("res/fig/ppl-samples-", n, ".png", sep = ""), res = 300, 
    height = 150, width = 150, units = 'mm')
# lattice::trellis.par.set(fontsize = list(text = 14, points = 12))  # PhD thesis defence
plot_samples(samples = list(sample_a, sample_b, sample_c, sample_d),
             # phd = TRUE, # PhD thesis defence
             n = n)
dev.off()
```

The box-and-whisker plots also show that when the criterion is the number of 
points per lag-distance class, random and systematic sampling produce an 
insignificant number of points contributing to the first lag-distance classes,
the last being the worst. The coverage of these lags is better when the sample
configuration is optimized regarding the number of point-pairs per lag-distance
class. All three sample types produce a more satisfactory coverage of the last
four lags as the sample size increases.

With the sample configurations at hand, we sample the simulated realities. The 
sampled point data is used to calibrate linear mixed models using the R-package
`geoR`. Model parameters (nugget, partial sill, and range) are estimated using
restricted maximum likelihood (REML).

```{r}
# Sample the simulated realities
sample_a <- sample_simulated_realities(samples = sample_a, realities = sim_real)
sample_b <- sample_simulated_realities(samples = sample_b, realities = sim_real)
sample_c <- sample_simulated_realities(samples = sample_c, realities = sim_real)
sample_d <- sample_simulated_realities(samples = sample_d, realities = sim_real)
# Calibrate linear mixed models
sample_a_lmm <- calibrate_lmm(samples = sample_a)
sample_b_lmm <- calibrate_lmm(samples = sample_b)
sample_c_lmm <- calibrate_lmm(samples = sample_c)
sample_d_lmm <- calibrate_lmm(samples = sample_d)
```

Both optimization criteria produce sample configurations that are efficient in
estimating the nugget variance regardless of the sample size, the number of 
points outperforming the number of point-pairs. Their efficiency is larger when
the range is moderate (50 m) to long (500 m). For a short range (5 m), both are
somewhat efficient only when the nugget variance is small (0.1).

Estimating the partial sill was somewhat difficult for all sample sizes. In 
general, using the number of points per lag-distance class as the optimization
criterion was more efficient than the number of point-pairs. More than three
samples need to be used to obtain more consistent conclusions.

The range was better estimated as the sample size increased, and when it is
short to moderate reardless of the sample configuration. For small sample sizes
(50), using the number of points per lag-distance class as the optimization
criterion was more efficient to estimate the range (5 and 50 m) when the nugget
variance was modetrate to low. No sample configuration made an accurate estimate
of the range when the nugget variance is high (0.9), or when the range is long
(500).

Overall, the random sample was completelly ineficient, while the efficiency of
the systematic sample increased with the sample size.

```{r}
# Compare the estimated parameters with the true parameters
sample_a_lmm <- data.frame(sample = "points", models, sample_a_lmm)
sample_b_lmm <- data.frame(sample = "point-pairs", models, sample_b_lmm)
sample_c_lmm <- data.frame(sample = "random", models, sample_c_lmm)
sample_d_lmm <- data.frame(sample = "systematic", models, sample_d_lmm)
sample_all <- rbind(sample_a_lmm, sample_b_lmm, sample_c_lmm, sample_d_lmm)
sample_all <- sample_all[, -2]
x11()
n <- 50
dev.off()
pdf(paste("res/fig/nugget-", n, "pts.pdf", sep = ""))
plot_nugget(sample_all, n = n)
dev.off()
pdf(paste("res/fig/partial-sill-", n, "pts.pdf", sep = ""))
plot_partial_sill(sample_all, n = n)
dev.off()
pdf(paste("res/fig/range-", n, "pts.pdf", sep = ""))
plot_range(sample_all, n = n)
dev.off()
```


[ref01]: https://sites.google.com/site/pedometrics2015
[ref02]: https://en.wikipedia.org/wiki/Pythagorean_theorem
[ref03]: https://en.wikipedia.org/wiki/Circumscribed_circle
[ref04]: http://www.sciencedirect.com/science/article/pii/S0016706101000921
[ref05]: http://dx.doi.org/10.1111/j.1365-2389.1992.tb00128.x
